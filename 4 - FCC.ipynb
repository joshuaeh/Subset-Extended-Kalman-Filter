{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCC-Fractionator\n",
    "\n",
    "References:\n",
    "\n",
    "a. Santander (2022) An open source fluid catalytic cracker - fractionator model to support the development and benchmarking of process control, machine learning and operation strategies [Computers and Chemical Engineering]  \n",
    "b. Santander (2023) Deep Learning Model Predictive Control Frameworks: Application to a Fluid Catalytic Cracker-Fractionator Process [I&EC research]  \n",
    "c. Santander (2023) Integrated Production Planning and Model Predictive Control of a Fluidized Bed Catalytic Cracking-Fractionator Unit [I&EC research]  \n",
    "d. Kumar (2023) Fluid Catalytic Cracking Unit Dataset for Process Monitoring Evaluation [github](https://github.com/ML-PSE/Fluid-Catalytic-Cracking-Unit-Dataset-for-Process-Monitoring-Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing as skp \n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.func import functional_call\n",
    "\n",
    "from utils import *\n",
    "from utils.fcc import *\n",
    "from utils.optimizers import maskedSGD, GEKF, maskedAdam\n",
    "from utils.modeling import *\n",
    "\n",
    "results_dir = os.path.join(\"results\", \"fcc\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "%config InlineBackend.figure_format = \"png\"\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "MEASUREMENT_NOISE_LEVEL = 0.0025  # +/- of the mean value of each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Process Data\n",
    " import & visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Operating Conditions FCC Data\n",
    "if 0:\n",
    "    df_stable = pd.read_csv(NOC_STABLE_PATH, names=COLUMNS)\n",
    "    df_varying = pd.read_csv(NOC_VARYING_PATH, names=COLUMNS)\n",
    "    df = pd.concat([df_stable, df_varying], ignore_index=True)\n",
    "\n",
    "    dfplot = df[sorted_columns]\n",
    "    data = default_rng.standard_normal(dfplot.shape) * MEASUREMENT_NOISE_LEVEL/3 * dfplot.values.mean(axis=0) + dfplot.values\n",
    "    dfplot = pd.DataFrame(data, columns=sorted_columns)\n",
    "    \n",
    "    fig, axs = graph_subplots(dfplot)\n",
    "\n",
    "    # fig.savefig(os.path.join(results_dir, \"NOC_data.png\"), bbox_inches=\"tight\", dpi=500)\n",
    "    # fig.savefig(os.path.join(results_dir, \"NOC_data.pdf\"), bbox_inches=\"tight\", dpi=1_000)\n",
    "    # fig.savefig(os.path.join(results_dir, \"NOC_data.svg\"), bbox_inches=\"tight\", dpi=500)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    scaler = skp.RobustScaler()\n",
    "    dfplot_scaled = pd.DataFrame(scaler.fit_transform(dfplot), columns=sorted_columns)\n",
    "    \n",
    "    fig, axs = graph_subplots(dfplot_scaled)\n",
    "    # fig.savefig(os.path.join(results_dir, \"NOC_data_scaled.png\"), bbox_inches=\"tight\", dpi=500)\n",
    "    # fig.savefig(os.path.join(results_dir, \"NOC_data_scaled.pdf\"), bbox_inches=\"tight\", dpi=1_000)\n",
    "    # fig.savefig(os.path.join(results_dir, \"NOC_data_scaled.svg\"), bbox_inches=\"tight\", dpi=500)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faulty operation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faulty Operating Conditions FCC Data\n",
    "if 0:\n",
    "    sorted_columns = sorted(X_COLUMNS_NAMES) + sorted(U_COLUMNS_NAMES)\n",
    "\n",
    "    df_stable = pd.read_csv(NOC_STABLE_PATH, names=COLUMNS)\n",
    "    \n",
    "    for k,v in FAULTY_CONDITIONS.items():\n",
    "        df_faulty = pd.read_csv(v[\"path\"], names=COLUMNS)\n",
    "        df = pd.concat([df_stable, df_faulty], ignore_index=True)\n",
    "\n",
    "        dfplot = df[sorted_columns]\n",
    "        \n",
    "        # add noise to the data\n",
    "        data = default_rng.standard_normal(dfplot.shape) * MEASUREMENT_NOISE_LEVEL/3 * dfplot.values.mean(axis=0) + dfplot.values\n",
    "        dfplot = pd.DataFrame(data, columns=sorted_columns)\n",
    "\n",
    "        fig, axs = graph_subplots(dfplot)\n",
    "\n",
    "        fig.savefig(os.path.join(results_dir, f\"FOC_{k}_data.png\"), bbox_inches=\"tight\", dpi=500)\n",
    "        fig.savefig(os.path.join(results_dir, f\"FOC_{k}_data.pdf\"), bbox_inches=\"tight\", dpi=1_000)\n",
    "        fig.savefig(os.path.join(results_dir, f\"FOC_{k}_data.svg\"), bbox_inches=\"tight\", dpi=500)\n",
    "        \n",
    "        dfplot_scaled = pd.DataFrame(scaler.transform(dfplot), columns=sorted_columns)\n",
    "        \n",
    "        fig, axs = graph_subplots(dfplot_scaled)\n",
    "        \n",
    "        fig.savefig(os.path.join(results_dir, f\"FOC_{k}_data_scaled.png\"), bbox_inches=\"tight\", dpi=500)\n",
    "        fig.savefig(os.path.join(results_dir, f\"FOC_{k}_data_scaled.pdf\"), bbox_inches=\"tight\", dpi=1_000)\n",
    "        fig.savefig(os.path.join(results_dir, f\"FOC_{k}_data_scaled.svg\"), bbox_inches=\"tight\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training, validation, and test datasets\n",
    "\n",
    "Add noise  \n",
    "Type K thermocouples have are accurate to 2.2C or +/- 0.75%, whichever is greater. Assume this is true of all sensors: that they are within 1% in a Gaussian distribution. Also assume that this is incorporated in 3 standard deviations from the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VanillaRNN(state_dim=len(X_COLUMNS), input_dim=len(U_COLUMNS), hidden_size=64)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=50, T_mult=1)\n",
    "\n",
    "# lrs = []\n",
    "# for e in range(200):\n",
    "#     lrs.append(opt.param_groups[0][\"lr\"])\n",
    "#     lr_scheduler.step()\n",
    "    \n",
    "# fig, ax = plt.subplots(figsize=(5, 3), dpi=300)\n",
    "\n",
    "# ax.plot(lrs)\n",
    "# ax.set_ylabel(\"Learning Rate\")\n",
    "# ax.set_xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df_stable = pd.read_csv(NOC_STABLE_PATH, names=COLUMNS)\n",
    "df_varying = pd.read_csv(NOC_VARYING_PATH, names=COLUMNS)\n",
    "df = pd.concat([df_stable, df_varying], ignore_index=True)\n",
    "# split and scale data\n",
    "# 7 days train (2 stable, 5 varying) ~78%\n",
    "# 1 day val ~11%\n",
    "# 1 day test ~11%\n",
    "train_val_idx = -2880\n",
    "val_test_idx = -1440\n",
    "prediction_horizon = 60\n",
    "batch_size = 32\n",
    "\n",
    "if MEASUREMENT_NOISE_LEVEL:\n",
    "    noise_std = MEASUREMENT_NOISE_LEVEL / 3\n",
    "    # Add Gaussian noise with st dev of 1/3% of the mean value of each variable\n",
    "    data = default_rng.standard_normal(df.shape) * MEASUREMENT_NOISE_LEVEL/3 * df.values.mean(axis=0) + df.values\n",
    "\n",
    "    df = pd.DataFrame(data, columns=df.columns)\n",
    "\n",
    "# X_scaler = skp.QuantileTransformer(output_distribution='normal')\n",
    "# U_scaler = skp.QuantileTransformer(output_distribution='normal')\n",
    "# X_scaler = skp.RobustScaler(quantile_range=(10, 90))\n",
    "# U_scaler = skp.RobustScaler(quantile_range=(10, 90))\n",
    "# X_scaler = skp.MaxAbsScaler()\n",
    "# U_scaler = skp.MaxAbsScaler()\n",
    "# X_scaler = StandardScaler()\n",
    "# U_scaler = StandardScaler()\n",
    "X_scaler = skp.RobustScaler()\n",
    "U_scaler = skp.RobustScaler()\n",
    "\n",
    "train_dataset = FCUDataset(df, \n",
    "    prediction_horizon=prediction_horizon, \n",
    "    context_length=1, \n",
    "    train=True, \n",
    "    X_scaler=X_scaler, \n",
    "    U_scaler=U_scaler, \n",
    "    begin_split_index=0, \n",
    "    end_split_index=train_val_idx)\n",
    "val_dataset = FCUDataset(df,\n",
    "    prediction_horizon=prediction_horizon, \n",
    "    context_length=1, \n",
    "    train=False, \n",
    "    X_scaler=X_scaler, \n",
    "    U_scaler=U_scaler, \n",
    "    begin_split_index=train_val_idx, \n",
    "    end_split_index=val_test_idx)\n",
    "test_dataset = FCUDataset(df,\n",
    "    prediction_horizon=prediction_horizon, \n",
    "    context_length=1, \n",
    "    train=False, \n",
    "    X_scaler=X_scaler, \n",
    "    U_scaler=U_scaler, \n",
    "    begin_split_index=val_test_idx)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=128, shuffle=True, generator=torch.Generator(device=device))\n",
    "val_dl = DataLoader(val_dataset, batch_size=128, shuffle=False, generator=torch.Generator(device=device))\n",
    "test_dl = DataLoader(test_dataset, batch_size=128, shuffle=False, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models\n",
    "\n",
    "Vanilla RNN - Adam\n",
    "\n",
    "train loss: 0.1663  \n",
    "val loss: 0.1709  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RKRNN - Adam\n",
    "\n",
    "Train loss: 0.1681  \n",
    "Val Loss: 0.1724  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating\n",
    "\n",
    "get gradients on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfilename = os.path.join(results_dir, \"Faulty Conditions\", \"FC_GEKFv2.h5\")\n",
    "logger = h5_logger(logfilename, existing=True)\n",
    "rk_trained_weights_path = os.path.join(results_dir, \"training\", \"RkRNN_NOC.pth\")\n",
    "\n",
    "if not check_if_in_h5(logfilename, \"training\"):\n",
    "    val_dl_individual = DataLoader(val_dataset, batch_size=1, shuffle=False, generator=torch.Generator(device=device))\n",
    "\n",
    "    m = Exogenous_RkRNN(state_dim=len(X_COLUMNS), input_dim=len(U_COLUMNS), hidden_size=64)\n",
    "    m.load_state_dict(torch.load(rk_trained_weights_path, map_location=device))\n",
    "    opt = maskedSGD(m.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    logger = h5_logger(logfilename, existing=True)\n",
    "\n",
    "\n",
    "    pbar = tqdm(val_dl_individual)\n",
    "    for data in pbar:\n",
    "        opt.zero_grad()\n",
    "        X0, U, X1 = data[\"X0\"], data[\"U\"], data[\"X1\"]\n",
    "        xp = m(X0, U)\n",
    "        loss = loss_fn(xp, X1)\n",
    "        loss.backward()\n",
    "        grads = opt._get_flat_grads()\n",
    "        logger.log_dict({\n",
    "            \"training/X0\": X0.detach().cpu().numpy(),\n",
    "            \"training/U\": U.detach().cpu().numpy(),\n",
    "            \"training/Y\": X1.detach().cpu().numpy(),\n",
    "            \"training/YP\": xp.detach().cpu().numpy(),\n",
    "            \"training/mse_scaled\": np.array([loss.item()]),\n",
    "            \"training/grads\": grads.detach().cpu().numpy()\n",
    "        })\n",
    "\n",
    "if check_if_in_h5(logfilename, \"training/abs_grad_values\"):\n",
    "    grad_values = logger.get_dataset(\"training/abs_grad_values\")[0]\n",
    "else:\n",
    "    grads = logger.get_dataset(\"training/grads\")\n",
    "    abs_grads = np.abs(grads).reshape(-1)\n",
    "    quantiles = np.linspace(0, 1, 101)\n",
    "    grad_values = [np.quantile(abs_grads, i) for i in quantiles]\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "# ax.step(quantiles, grad_values, where=\"post\")\n",
    "# ax.set_xlabel(\"Quantile\")\n",
    "# ax.set_ylabel(\"Gradient Magnitude\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate datasets\n",
    "\n",
    "We do this once then draw from it so that the random noise is the same each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_KEYS = list(FAULTY_CONDITIONS.keys())\n",
    "\n",
    "for fc_key in FC_KEYS:\n",
    "    log_header = f\"updating/{fc_key}/data/\"\n",
    "    if not check_if_in_h5(logfilename, log_header+\"df_data\"):\n",
    "        faulty_condition = FAULTY_CONDITIONS[fc_key]\n",
    "        df = pd.read_csv(faulty_condition[\"path\"], names=COLUMNS)\n",
    "        df_data = default_rng.standard_normal(df.shape) * MEASUREMENT_NOISE_LEVEL/3 * df.values.mean(axis=0) + df.values\n",
    "        logger.log_value(log_header+\"df_data\", df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Exogenous_RkRNN(state_dim=len(X_COLUMNS), input_dim=len(U_COLUMNS), hidden_size=64)\n",
    "m.load_state_dict(torch.load(rk_trained_weights_path, map_location=device, weights_only=True))\n",
    "\n",
    "FC_KEYS = list(FAULTY_CONDITIONS.keys())\n",
    "for fc_key in FC_KEYS:\n",
    "    faulty_condition = FAULTY_CONDITIONS[fc_key]\n",
    "    log_header = f\"updating/{fc_key}/control/\"\n",
    "    if not check_if_in_h5(logfilename, log_header):\n",
    "        data = logger.get_dataset(f\"updating/{fc_key}/data/df_data\")[0]\n",
    "        df = pd.DataFrame(data, columns=COLUMNS)\n",
    "        faulty_dataset = FCUDataset(df,\n",
    "            X_scaler=X_scaler,\n",
    "            U_scaler=U_scaler)\n",
    "        faulty_dataloader = DataLoader(faulty_dataset, batch_size=128, shuffle=False, generator=torch.Generator(device=device))\n",
    "\n",
    "        # try without updating the weights\n",
    "        m.eval()\n",
    "        faulty_data = flatten_predictions(m, faulty_dataloader)\n",
    "        \n",
    "        # mae and mse between each prediction and the actual values\n",
    "        X0, U, Y, YP = faulty_data\n",
    "        mae = np.abs(Y-YP).mean(axis=-1).mean(axis=-1)\n",
    "        mse = ((Y-YP)**2).mean(axis=-1).mean(axis=-1)\n",
    "        # scale\n",
    "        Y_scaled = X_scaler.transform(Y.reshape(-1, len(X_COLUMNS))).reshape(Y.shape)\n",
    "        YP_scaled = X_scaler.transform(YP.reshape(-1, len(X_COLUMNS))).reshape(YP.shape)\n",
    "        mae_scaled = np.abs(Y_scaled-YP_scaled).mean(axis=-1).mean(axis=-1)\n",
    "        mse_scaled = ((Y_scaled-YP_scaled)**2).mean(axis=-1).mean(axis=-1)\n",
    "        \n",
    "        logger = h5_logger(os.path.join(results_dir,\"Faulty Conditions\", \"FC_GEKF.h5\"), existing=True)\n",
    "        logger.log_dict({\n",
    "            f\"{log_header}/X0\": X0,\n",
    "            f\"{log_header}/U\": U,\n",
    "            f\"{log_header}/Y\": Y,\n",
    "            f\"{log_header}/YP\": YP,\n",
    "            f\"{log_header}/mae\": mae,\n",
    "            f\"{log_header}/mse\": mse,\n",
    "            f\"{log_header}/mae_scaled\": mae_scaled,\n",
    "            f\"{log_header}/mse_scaled\": mse_scaled\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KF Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fc_keys = FAULTY_CONDITIONS.keys()\n",
    "\n",
    "\n",
    "for fc_key in fc_keys:\n",
    "    trial_args = [\n",
    "        {\"log_header\": f\"updating/{fc_key}/full/\"},\n",
    "        {\"log_header\": f\"updating/{fc_key}/t-0.99/\", \"thresh\": grad_values[-2]}, \n",
    "        {\"log_header\": f\"updating/{fc_key}/t-0.95/\", \"thresh\": grad_values[-6]}, \n",
    "        {\"log_header\": f\"updating/{fc_key}/q-0.99/\", \"quantile_thresh\": 0.99},\n",
    "        {\"log_header\": f\"updating/{fc_key}/q-0.95/\", \"quantile_thresh\": 0.95},\n",
    "    ]\n",
    "    for args in trial_args:\n",
    "        sefk_updating_trial(fc_key, **args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM UPDATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 3e-4\n",
    "loss_threshold = 0.18\n",
    "\n",
    "fc_keys = FAULTY_CONDITIONS.keys()\n",
    "\n",
    "for fc_key in fc_keys:\n",
    "    trial_args = [\n",
    "        {\"log_header\": f\"retraining/{fc_key}/full/\"},  # all converged\n",
    "        {\"log_header\": f\"retraining/{fc_key}/t-0.99/\", \"thresh\": grad_values[-2]}, \n",
    "        {\"log_header\": f\"retraining/{fc_key}/t-0.95/\", \"thresh\": grad_values[-6]}, \n",
    "        {\"log_header\": f\"retraining/{fc_key}/q-0.99/\", \"quantile_thresh\": 0.99},\n",
    "        {\"log_header\": f\"retraining/{fc_key}/q-0.95/\", \"quantile_thresh\": 0.95},  # all converged\n",
    "    ]\n",
    "    for args in trial_args:\n",
    "        retraining_trial(fc_key,\n",
    "            lr=LR, loss_threshold=loss_threshold, early_stopping_threshold=0.15, divergence_threshold=20.0,\n",
    "            moving_horizon_length=20, retraining_epochs=5,\n",
    "            **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faulty condition</th>\n",
       "      <th>full</th>\n",
       "      <th>t-0.99</th>\n",
       "      <th>t-0.95</th>\n",
       "      <th>q-0.99</th>\n",
       "      <th>q-0.95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heat Exchanger Fouling</td>\n",
       "      <td>0.176112</td>\n",
       "      <td>0.180545</td>\n",
       "      <td>0.179543</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>0.181208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decreased Condenser Efficiency</td>\n",
       "      <td>0.170402</td>\n",
       "      <td>0.171087</td>\n",
       "      <td>0.170971</td>\n",
       "      <td>0.171120</td>\n",
       "      <td>0.170771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heavy Naptha Flow Sensor Drift</td>\n",
       "      <td>0.169325</td>\n",
       "      <td>0.170602</td>\n",
       "      <td>0.170382</td>\n",
       "      <td>0.170539</td>\n",
       "      <td>0.170601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Higher Pressure Drop</td>\n",
       "      <td>0.299892</td>\n",
       "      <td>0.384869</td>\n",
       "      <td>0.377844</td>\n",
       "      <td>0.418643</td>\n",
       "      <td>0.400323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAB Valve Leak</td>\n",
       "      <td>0.195650</td>\n",
       "      <td>0.220223</td>\n",
       "      <td>0.217721</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.224943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 faulty condition      full    t-0.99    t-0.95    q-0.99  \\\n",
       "0          Heat Exchanger Fouling  0.176112  0.180545  0.179543  0.185311   \n",
       "1  Decreased Condenser Efficiency  0.170402  0.171087  0.170971  0.171120   \n",
       "2  Heavy Naptha Flow Sensor Drift  0.169325  0.170602  0.170382  0.170539   \n",
       "3            Higher Pressure Drop  0.299892  0.384869  0.377844  0.418643   \n",
       "4                  CAB Valve Leak  0.195650  0.220223  0.217721  0.232100   \n",
       "\n",
       "     q-0.95  \n",
       "0  0.181208  \n",
       "1  0.170771  \n",
       "2  0.170601  \n",
       "3  0.400323  \n",
       "4  0.224943  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retraining_df = get_retraining_stats(logger)\n",
    "retraining_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faulty condition</th>\n",
       "      <th>full</th>\n",
       "      <th>t-0.99</th>\n",
       "      <th>t-0.95</th>\n",
       "      <th>q-0.99</th>\n",
       "      <th>q-0.95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heat Exchanger Fouling</td>\n",
       "      <td>0.168328</td>\n",
       "      <td>0.174741</td>\n",
       "      <td>0.172824</td>\n",
       "      <td>0.176612</td>\n",
       "      <td>0.173977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decreased Condenser Efficiency</td>\n",
       "      <td>0.164993</td>\n",
       "      <td>0.169433</td>\n",
       "      <td>0.168131</td>\n",
       "      <td>0.168998</td>\n",
       "      <td>0.167801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heavy Naptha Flow Sensor Drift</td>\n",
       "      <td>0.164849</td>\n",
       "      <td>0.168257</td>\n",
       "      <td>0.167596</td>\n",
       "      <td>0.168191</td>\n",
       "      <td>0.167368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Higher Pressure Drop</td>\n",
       "      <td>0.725912</td>\n",
       "      <td>0.597922</td>\n",
       "      <td>0.783820</td>\n",
       "      <td>0.353705</td>\n",
       "      <td>0.416279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAB Valve Leak</td>\n",
       "      <td>0.210221</td>\n",
       "      <td>0.253780</td>\n",
       "      <td>0.271964</td>\n",
       "      <td>0.210080</td>\n",
       "      <td>0.314612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 faulty condition      full    t-0.99    t-0.95    q-0.99  \\\n",
       "0          Heat Exchanger Fouling  0.168328  0.174741  0.172824  0.176612   \n",
       "1  Decreased Condenser Efficiency  0.164993  0.169433  0.168131  0.168998   \n",
       "2  Heavy Naptha Flow Sensor Drift  0.164849  0.168257  0.167596  0.168191   \n",
       "3            Higher Pressure Drop  0.725912  0.597922  0.783820  0.353705   \n",
       "4                  CAB Valve Leak  0.210221  0.253780  0.271964  0.210080   \n",
       "\n",
       "     q-0.95  \n",
       "0  0.173977  \n",
       "1  0.167801  \n",
       "2  0.167368  \n",
       "3  0.416279  \n",
       "4  0.314612  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updating_df = get_updating_stats(logger)\n",
    "updating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c62f86eb7c4e1aadda269adef8d2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "updating/Heat Exchanger Fouling/full_1/:   0%|          | 0/1378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_selection_methods = [\"full\", \"t-0.99\", \"t-0.95\", \"q-0.99\", \"q-0.95\"]\n",
    "retraining_less = retraining_df[param_selection_methods].values < updating_df[param_selection_methods].values\n",
    "for fc_key in list(fc_keys)[:-2]:\n",
    "    trial_args = [\n",
    "        {\"log_header\": f\"updating/{fc_key}/full/\"},\n",
    "        {\"log_header\": f\"updating/{fc_key}/t-0.99/\", \"thresh\": grad_values[-2]}, \n",
    "        {\"log_header\": f\"updating/{fc_key}/t-0.95/\", \"thresh\": grad_values[-6]}, \n",
    "        {\"log_header\": f\"updating/{fc_key}/q-0.99/\", \"quantile_thresh\": 0.99},\n",
    "        {\"log_header\": f\"updating/{fc_key}/q-0.95/\", \"quantile_thresh\": 0.95},\n",
    "    ]\n",
    "    for args in trial_args:\n",
    "        sefk_updating_trial(fc_key, find_unique_header= True, lr=2e-6, **args)\n",
    "# for fc_key in list(fc_keys)[-1:]:\n",
    "#     trial_args = [\n",
    "#     #     {\"log_header\": f\"updating/{fc_key}/full/\"},\n",
    "#     #     {\"log_header\": f\"updating/{fc_key}/t-0.99/\", \"thresh\": grad_values[-2]}, \n",
    "#     #     {\"log_header\": f\"updating/{fc_key}/t-0.95/\", \"thresh\": grad_values[-6]}, \n",
    "#     #     {\"log_header\": f\"updating/{fc_key}/q-0.99/\", \"quantile_thresh\": 0.99},\n",
    "#         {\"log_header\": f\"updating/{fc_key}/q-0.95/\", \"quantile_thresh\": 0.95},\n",
    "#     ]\n",
    "#     for args in trial_args:\n",
    "#         sefk_updating_trial(fc_key, find_unique_header= True, lr=2e-6, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph results\n",
    "\n",
    "- error over time\n",
    "- predictions\n",
    "- weight evolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in FC_KEYS:\n",
    "    base_path = os.path.join(results_dir,\"Faulty Conditions\", key)\n",
    "    base_key = f\"updating/{key}/\"\n",
    "    \n",
    "    control_group = base_key+\"control/\"\n",
    "    full_group = base_key+\"full/\"\n",
    "    threshold_group = base_key+\"threshold/\"\n",
    "    quantile_group = base_key+\"quantile0.99/\"\n",
    "    \n",
    "    ## Ensure that mae, mae_scaled, mse, mse_scaled are calculated for each group\n",
    "    # for group in [full_group, threshold_group, quantile_group]:\n",
    "    #     print(f\"group: {group}\")\n",
    "    #     # for error in [\"mae\", \"mae_scaled\", \"mse\", \"mse_scaled\"]:\n",
    "    #     #     logger.rm_key(group+error)\n",
    "    #     X1 = logger.get_dataset(group+\"X1\")\n",
    "    #     XP = logger.get_dataset(group+\"XP\")\n",
    "        \n",
    "    #     X1_rescaled = X_scaler.inverse_transform(X1.reshape(-1, len(X_COLUMNS))).reshape(X1.shape)\n",
    "    #     XP_rescaled = X_scaler.inverse_transform(XP.reshape(-1, len(X_COLUMNS))).reshape(XP.shape)\n",
    "        \n",
    "    #     # mae\n",
    "    #     if not check_if_in_h5(logfilename, group+\"mae\"):\n",
    "    #         mae = np.abs(X1_rescaled-XP_rescaled).mean(axis=-1).mean(axis=-1)\n",
    "    #         logger.log_dict({\n",
    "    #             group+\"mae\": mae\n",
    "    #         })\n",
    "        \n",
    "    #     # mae_scaled\n",
    "    #     if not check_if_in_h5(logfilename, group+\"mae_scaled\"):\n",
    "            \n",
    "    #         mae_scaled = np.abs(X1-XP).mean(axis=-1).mean(axis=-1)\n",
    "    #         logger.log_dict({\n",
    "    #             group+\"mae_scaled\": mae_scaled\n",
    "    #         })\n",
    "        \n",
    "    #     # mse\n",
    "    #     if not check_if_in_h5(logfilename, group+\"mse\"):\n",
    "    #         mse = ((X1_rescaled-XP_rescaled)**2).mean(axis=-1).mean(axis=-1)\n",
    "    #         logger.log_dict({\n",
    "    #             group+\"mse\": mse\n",
    "    #         })\n",
    "        \n",
    "    #     # mse_scaled\n",
    "    #     if not check_if_in_h5(logfilename, group+\"mse_scaled\"):\n",
    "    #         mse_scaled = ((X1-XP)**2).mean(axis=-1).mean(axis=-1)\n",
    "    #         logger.log_dict({\n",
    "    #             group+\"mse_scaled\": mse_scaled\n",
    "    #         })\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.get_group_keys(\"updating/CAB Valve Leak/full/\")\n",
    "logger.get_dataset(\"updating/CAB Valve Leak/full/parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_condition = \"Higher Pressure Drop\"\n",
    "# pressure: reactor\n",
    "# pressure: delta\n",
    "p_delta_idx = X_COLUMNS_NAMES.index(\"P.delta\")\n",
    "p_reactor_idx = X_COLUMNS_NAMES.index(\"P.reactor\")\n",
    "selected_indices = [p_delta_idx, p_reactor_idx]\n",
    "\n",
    "true_data = np.concatenate([\n",
    "    logger.get_dataset(\"updating/\"+faulty_condition+\"/control/Y\")[0,:,0,[selected_indices]][0].T,\n",
    "    logger.get_dataset(\"updating/\"+faulty_condition+\"/control/Y\")[0,-1,1:,[selected_indices]][0].T\n",
    "]).T\n",
    "\n",
    "control_t10 = logger.get_dataset(\"updating/\"+faulty_condition+\"/control/YP\")[0,:,9,[selected_indices]][0]\n",
    "control_t60 = logger.get_dataset(\"updating/\"+faulty_condition+\"/control/YP\")[0,:,-1,[selected_indices]][0]\n",
    "\n",
    "kf_10 = X_scaler.inverse_transform(logger.get_dataset(\"updating/\"+faulty_condition+\"/full/YP\")[:,0,9,:]).T[selected_indices]\n",
    "kf_60 = X_scaler.inverse_transform(logger.get_dataset(\"updating/\"+faulty_condition+\"/full/YP\")[:,0,-1,:]).T[selected_indices]\n",
    "\n",
    "\n",
    "fig_hours = np.arange(true_data.shape[1])/60\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(13.3, 6), sharex=True, dpi=300)\n",
    "axs[0].plot(fig_hours, true_data[0], color=true_colors[0], linewidth=3)\n",
    "# axs[0].plot(np.arange(len(control_t10[0]))/60+10/60, control_t10[0], color=model_colors[0])\n",
    "# axs[0].plot(np.arange(len(control_t60[0]))/60+1, control_t60[0], color=model_colors[2], linestyle=\"-\")\n",
    "# axs[0].plot(np.arange(len(kf_10[0]))/60+10/60, kf_10[0], color=kalman_colors[0])\n",
    "# axs[0].plot(np.arange(len(kf_60[0]))/60+1, kf_10[0], color=kalman_colors[2], linestyle=\"-\")\n",
    "axs[0].set_title(r\"Pressure: Delta (psi$_g$)\")\n",
    "axs[0].yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "axs[0].set_xlim(-0.5, 24.5)\n",
    "true, = axs[1].plot(fig_hours, true_data[1], color=true_colors[0], linewidth=3)\n",
    "# original_t10, = axs[1].plot(np.arange(len(control_t10[1]))/60+10/60, control_t10[1], color=model_colors[0])\n",
    "# original_t60, = axs[1].plot(np.arange(len(control_t60[1]))/60+1, control_t60[1], color=model_colors[2], linestyle=\"-\")\n",
    "# kf_t10, = axs[1].plot(np.arange(len(kf_10[1]))/60+10/60, kf_10[1], color=kalman_colors[0])\n",
    "# kf_t60, = axs[1].plot(np.arange(len(kf_60[1]))/60+1, kf_10[1], color=kalman_colors[2], linestyle=\"-\")\n",
    "axs[1].set_title(r\"Pressure: Reactor (psi$_g$)\")\n",
    "axs[1].yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "axs[1].set_xlim(-0.5, 24.5)\n",
    "axs[1].set_xlabel(\"Time (hours)\")\n",
    "axs[1].set_xticks([0, 4, 8, 12, 16, 20, 24])\n",
    "axs[1].set_xticks(range(25), minor=True)\n",
    "# axs[1].legend(\n",
    "#     [true, original_t10, original_t60, kf_t10, kf_t60],\n",
    "#     [\"True\", r\"Original$_{t+10}$\", r\"Original$_{t+60}$\", r\"KF$_{t+10}$\", r\"KF$_{t+60}$\"], loc=\"upper center\",\n",
    "#     bbox_to_anchor=(0.5, -0.36), ncol=5)\n",
    "axs[1].legend([true], [\"True\"], bbox_to_anchor=(0.5, -0.36), ncol=1, loc=\"upper center\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Higher Pressure Drop\", \"Pressure Drop Blank.png\"), bbox_inches=\"tight\", dpi=300)\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Higher Pressure Drop\", \"Pressure Drop Blank.svg\"), bbox_inches=\"tight\", dpi=300)\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Higher Pressure Drop\", \"Pressure Drop Blank.pdf\"), bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "control_error = logger.get_dataset(\"updating/\"+faulty_condition+\"/control/mse_scaled\")[0]\n",
    "kf_full_error = logger.get_dataset(\"updating/\"+faulty_condition+\"/full/loss\")\n",
    "kf_q95_error = logger.get_dataset(\"updating/\"+faulty_condition+\"/q-0.95/loss\")\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(13.3, 6), sharex=True, dpi=300)\n",
    "axs[0].plot(fig_hours, true_data[0], color=true_colors[0], linewidth=3)\n",
    "axs[0].plot(np.arange(len(control_t10[0]))/60+10/60, control_t10[0], color=model_colors[0])\n",
    "axs[0].plot(np.arange(len(control_t60[0]))/60+1, control_t60[0], color=model_colors[2], linestyle=\"-\")\n",
    "axs[0].plot(np.arange(len(kf_10[0]))/60+10/60, kf_10[0], color=kalman_colors[0])\n",
    "axs[0].plot(np.arange(len(kf_60[0]))/60+1, kf_10[0], color=kalman_colors[2], linestyle=\"-\")\n",
    "axs[0].set_title(r\"Pressure: Delta (psi$_g$)\")\n",
    "axs[0].yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "axs[0].set_xlim(-0.5, 24.5)\n",
    "true, = axs[1].plot(fig_hours, true_data[1], color=true_colors[0], linewidth=3)\n",
    "original_t10, = axs[1].plot(np.arange(len(control_t10[1]))/60+10/60, control_t10[1], color=model_colors[0])\n",
    "original_t60, = axs[1].plot(np.arange(len(control_t60[1]))/60+1, control_t60[1], color=model_colors[2], linestyle=\"-\")\n",
    "kf_t10, = axs[1].plot(np.arange(len(kf_10[1]))/60+10/60, kf_10[1], color=kalman_colors[0])\n",
    "kf_t60, = axs[1].plot(np.arange(len(kf_60[1]))/60+1, kf_10[1], color=kalman_colors[2], linestyle=\"-\")\n",
    "axs[1].set_title(r\"Pressure: Reactor (psi$_g$)\")\n",
    "axs[1].yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "axs[1].set_xlim(-0.5, 24.5)\n",
    "# axs[2].plot(np.arange(len(control_error))/60, control_error, color=model_colors[0])\n",
    "# axs[2].plot(np.arange(len(kf_full_error))/60, kf_full_error, color=kalman_colors[0])\n",
    "# # axs[2].plot(np.arange(len(kf_q95_error))/60, kf_q95_error, color=kalman_colors[2])\n",
    "# axs[2].set_title(\"Overall Prediction Mean Squared Error\")\n",
    "axs[1].set_xlabel(\"Time (hours)\")\n",
    "axs[1].set_xticks([0, 4, 8, 12, 16, 20, 24])\n",
    "axs[1].set_xticks(range(25), minor=True)\n",
    "axs[1].yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "axs[1].legend(\n",
    "    [true, original_t10, original_t60, kf_t10, kf_t60],\n",
    "    [\"True\", r\"Original$_{t+10}$\", r\"Original$_{t+60}$\", r\"KF$_{t+10}$\", r\"KF$_{t+60}$\"], loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.57), ncol=5)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Higher Pressure Drop\", \"Pressure Drop.png\"), bbox_inches=\"tight\", dpi=300)\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Higher Pressure Drop\", \"Pressure Drop.svg\"), bbox_inches=\"tight\", dpi=300)\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Higher Pressure Drop\", \"Pressure Drop.pdf\"), bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_condition = \"Higher Pressure Drop\"\n",
    "\n",
    "kf_full_time = np.diff(logger.get_dataset(\"updating/\"+faulty_condition+\"/full-v2/time\"), axis=0, prepend=0)\n",
    "kf_q95_time = np.diff(logger.get_dataset(\"updating/\"+faulty_condition+\"/q-0.95/time\"), axis=0, prepend=0)\n",
    "# kf_q98_time = np.diff(logger.get_dataset(\"updating/\"+faulty_condition+\"/q-0.98/time\"), axis=0, prepend=0)\n",
    "kf_q99_time = np.diff(logger.get_dataset(\"updating/\"+faulty_condition+\"/q-0.99/time\"), axis=0, prepend=0)\n",
    "kf_t95_time = np.diff(logger.get_dataset(\"updating/\"+faulty_condition+\"/t-0.95-v2/time\"), axis=0, prepend=0)\n",
    "# kf_t98_time = np.diff(logger.get_dataset(\"updating/\"+faulty_condition+\"/t-q98/time\"), axis=0, prepend=0)\n",
    "kf_t99_time = np.diff(logger.get_dataset(\"updating/\"+faulty_condition+\"/t-0.99-v2/time\"), axis=0, prepend=0)\n",
    "\n",
    "kf_full_loss = logger.get_dataset(\"updating/\"+faulty_condition+\"/full-v2/loss\")\n",
    "kf_q95_loss = logger.get_dataset(\"updating/\"+faulty_condition+\"/q-0.95/loss\")\n",
    "# kf_q98_loss = logger.get_dataset(\"updating/\"+faulty_condition+\"/quantile0.98/loss\")\n",
    "kf_q99_loss = logger.get_dataset(\"updating/\"+faulty_condition+\"/q-0.99/loss\")\n",
    "kf_t95_loss = logger.get_dataset(\"updating/\"+faulty_condition+\"/t-0.95-v2/loss\")\n",
    "# kf_t98_loss = logger.get_dataset(\"updating/\"+faulty_condition+\"/threshol-q98/loss\")\n",
    "kf_t99_loss = logger.get_dataset(\"updating/\"+faulty_condition+\"/t-0.99-v2/loss\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6), dpi=300)\n",
    "plot_ci(kf_full_loss, kf_full_time, ax, label=\"All\", color=kalman_colors[0])\n",
    "plot_ci(kf_q99_loss, kf_q99_time, ax, label=\"Quantile 0.99\", color=kalman_colors[0], marker_fmt=\"v\")\n",
    "plot_ci(kf_q95_loss, kf_q95_time, ax, label=\"Quantile 0.95\", color=kalman_colors[1], marker_fmt=\"^\")\n",
    "# plot_ci(kf_q98_time, kf_q98_loss, ax, label=\"Quantile 0.98\", color=kalman_colors[2], marker_fmt=\"v\")\n",
    "plot_ci(kf_t99_loss, kf_t99_time, ax, label=\"Threshold 0.99\", color=kalman_colors[0], marker_fmt=\"s\")\n",
    "plot_ci(kf_t95_loss, kf_t95_time, ax, label=\"Threshold 0.95\", color=kalman_colors[1], marker_fmt=\"D\")\n",
    "# plot_ci(kf_t98_loss, kf_t98_time, ax, label=\"Threshold (q98)\", color=adam_colors[1])\n",
    "ax.set_xlabel(\"Mean Squared Error\")\n",
    "ax.set_ylabel(\"Mean Time Per Iteration (s)\")\n",
    "\n",
    "legend_markers = [\n",
    "    Line2D([0], [0], color=kalman_colors[0], marker=\"o\", linestyle=\"None\"),\n",
    "    Line2D([0], [0], color=kalman_colors[0], marker=\"v\", linestyle=\"None\"),\n",
    "    Line2D([0], [0], color=kalman_colors[1], marker=\"^\", linestyle=\"None\"),\n",
    "    # Line2D([0], [0], color=kalman_colors[2], marker=\"v\", linestyle=\"None\"),\n",
    "    Line2D([0], [0], color=kalman_colors[0], marker=\"s\", linestyle=\"None\"),\n",
    "    Line2D([0], [0], color=kalman_colors[1], marker=\"D\", linestyle=\"None\"),\n",
    "]\n",
    "legend_labels = [\n",
    "    \"All\",\n",
    "    \"Quantile 0.99\",\n",
    "    \"Quantile 0.95\",\n",
    "    # \"Quantile 0.98\",\n",
    "    \"Threshold 0.99\",\n",
    "    \"Threshold 0.95\",\n",
    "]\n",
    "\n",
    "ax.legend(legend_markers, legend_labels)\n",
    "\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Higher Pressure Drop\", \"Time vs Loss.png\"), bbox_inches=\"tight\", dpi=300)\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Higher Pressure Drop\", \"Time vs Loss.svg\"), bbox_inches=\"tight\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heavy naptha flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stable = pd.read_csv(NOC_STABLE_PATH, names=COLUMNS)\n",
    "df_faulty = pd.read_csv(FAULTY_NAPTHASENSOR_PATH, names=COLUMNS)\n",
    "df = pd.concat([df_stable, df_faulty], ignore_index=True)\n",
    "data = default_rng.standard_normal(df.shape) * MEASUREMENT_NOISE_LEVEL/3 * df.values.mean(axis=0) + df.values\n",
    "df = pd.DataFrame(data, columns=COLUMNS)\n",
    "\n",
    "HN_idx = X_COLUMNS_NAMES.index(\"F.HN\")\n",
    "\n",
    "kf_30 = X_scaler.inverse_transform(logger.get_dataset(\"updating/\"+\"Heavy Naptha Flow Sensor Drift\"+\"/full/XP\")[:,0,30,:]).T[HN_idx]\n",
    "control_t30 = logger.get_dataset(\"updating/\"+faulty_condition+\"/control/YP\")[0,:,30,[HN_idx]][0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6), dpi=300)\n",
    "\n",
    "ax.plot(np.arange(df.shape[0])/60, df[\"F.HN\"], color=true_colors[0], label=\"Measured Data\")\n",
    "ax.plot(np.arange(kf_30.shape[0])/60+30/60+48, kf_30, color=kalman_colors[0], linewidth=1.5, label=r\"KF-Updated$_t+30$\")\n",
    "ax.plot(np.arange(control_t30.shape[0])/60+30/60+48, control_t30, color=model_colors[0], linewidth=1.5, label=r\"Original Model$_t+30$\")\n",
    "\n",
    "ax.vlines(48, min(df[\"F.HN\"])-10, max(df[\"F.HN\"])+10, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "ax.text(24, max(df[\"F.HN\"]), \"Normal Operating\\nConditions\", ha=\"center\")\n",
    "ax.text(60, max(df[\"F.HN\"]), \"Sensor\\nFault\", ha=\"center\")\n",
    "ax.set_xlabel(\"Time (hours)\")\n",
    "ax.set_xticks([0, 12, 24, 36, 48, 60, 72])\n",
    "ax.set_xticks(np.arange(72, step=4), minor=True)\n",
    "ax.legend(bbox_to_anchor=(0.5, -0.2), ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(13,4.5), dpi=300)\n",
    "\n",
    "ax.plot(np.arange(df.shape[0])/60, df[\"F.HN\"], color=true_colors[0], label=\"Measured Data\")\n",
    "ax.plot(np.arange(kf_30.shape[0])/60+30/60+48, kf_30, color=kalman_colors[0], linewidth=1.5, label=r\"KF-Updated$_{t+30}$\")\n",
    "ax.plot(np.arange(control_t30.shape[0])/60+30/60+48, control_t30, color=model_colors[0], linewidth=1.5, label=r\"Original Model$_{t+30}$\")\n",
    "\n",
    "ax.vlines(48, min(df[\"F.HN\"])-10, max(df[\"F.HN\"])+10, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "ax.text(24, max(df[\"F.HN\"]), \"Normal Operating\\nConditions\", ha=\"center\")\n",
    "ax.text(60, max(df[\"F.HN\"]), \"Sensor\\nFault\", ha=\"center\")\n",
    "ax.set_xlabel(\"Time (hours)\")\n",
    "ax.set_xticks([0, 12, 24, 36, 48, 60, 72])\n",
    "ax.set_xticks(np.arange(72, step=4), minor=True)\n",
    "ax.set_ylabel(\"Heavy Naptha Flow (lb/min)\")\n",
    "ax.legend(bbox_to_anchor=(0.5, -0.12), ncol=3, loc=\"upper center\")\n",
    "\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Heavy Naptha Flow Sensor Drift\", \"Heavy Naptha Flow.png\"), bbox_inches=\"tight\", dpi=300)\n",
    "fig.savefig(os.path.join(results_dir, \"Faulty Conditions\", \"Heavy Naptha Flow Sensor Drift\", \"Heavy Naptha Flow.svg\"), bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = h5_logger(os.path.join(results_dir,\"Faulty Conditions\", \"FC_GEKF.h5\"), existing=True)\n",
    "\n",
    "# for key in FC_KEYS:\n",
    "#     base_path = os.path.join(results_dir,\"Faulty Conditions\", key)\n",
    "#     base_key = f\"updating/{key}/\"\n",
    "    \n",
    "#     control_group = base_key+\"control/\"\n",
    "#     full_group = base_key+\"full/\"\n",
    "#     threshold_group = base_key+\"threshold/\"\n",
    "#     quantile_group = base_key+\"quantile0.99/\"\n",
    "    \n",
    "#     errors = {\n",
    "#         \"mae\": \"Mean Absolute Error\",\n",
    "#         \"mae_scaled\": \"Mean Absolute Error (Scaled)\",\n",
    "#         \"mse\": \"Mean Squared Error\",\n",
    "#         \"mse_scaled\": \"Mean Squared Error (Scaled)\"\n",
    "#     }\n",
    "    \n",
    "#     # graph error over time\n",
    "#     for error, name in errors.items():\n",
    "#         fig, ax = plt.subplots(1, 1, figsize=(12, 6), dpi=300)\n",
    "#         ax.plot(logger.get_dataset(control_group+error)[0], \"k-\", label=\"No Updates\")\n",
    "#         ax.plot(logger.get_dataset(full_group+error)[0], label=\"Update All Parameters\")\n",
    "#         ax.plot(logger.get_dataset(threshold_group+error)[0], label=fr\"Update Parameters $|\\nabla L| >$ {thresh:.4f}\")\n",
    "#         ax.plot(logger.get_dataset(quantile_group+error)[0], label=r\"Update Parameters $|\\nabla L| >$ 0.99 Quantile\")\n",
    "        \n",
    "#         ax.set_xlabel(\"Prediction Time\")\n",
    "#         ax.set_xticks(np.arange(0, 1441, 120))\n",
    "#         ax.set_xlim(0, 1440)\n",
    "#         ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x/60:.0f}:00\"))\n",
    "#         ax.set_ylabel(name)\n",
    "#         ax.set_title(f\"{key}: Error Over Time\")\n",
    "#         ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.075), ncol=2)\n",
    "        \n",
    "#         fig.savefig(os.path.join(base_path, f\"{error}.png\"), bbox_inches=\"tight\")\n",
    "#         fig.savefig(os.path.join(base_path, f\"{error}.svg\"), bbox_inches=\"tight\")\n",
    "#         plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 600\n",
    "# rescale = True\n",
    "# scale = 1\n",
    "# figsize = (32*scale, 16*scale)\n",
    "\n",
    "# for key in FC_KEYS:\n",
    "#     base_path = os.path.join(results_dir,\"Faulty Conditions\", key)\n",
    "#     base_key = f\"updating/{key}/\"\n",
    "    \n",
    "#     control_group = base_key+\"control/\"\n",
    "#     full_group = base_key+\"full/\"\n",
    "#     threshold_group = base_key+\"threshold/\"\n",
    "#     quantile_group = base_key+\"quantile0.99/\"\n",
    "    \n",
    "#     x0 = logger.get_dataset(control_group+\"X0\").squeeze()[index]\n",
    "#     u_vars = logger.get_dataset(control_group+\"U\").squeeze()[index]\n",
    "#     true = logger.get_dataset(control_group+\"Y\").squeeze()[index]\n",
    "#     control = logger.get_dataset(control_group+\"YP\").squeeze()[index]\n",
    "#     full = logger.get_dataset(full_group+\"XP\").squeeze()[index]\n",
    "#     threshold = logger.get_dataset(threshold_group+\"XP\").squeeze()[index]\n",
    "#     quantile = logger.get_dataset(quantile_group+\"XP\").squeeze()[index]\n",
    "    \n",
    "#     if rescale:\n",
    "#         full = X_scaler.inverse_transform(full.reshape(-1, len(X_COLUMNS))).reshape(full.shape)\n",
    "#         threshold = X_scaler.inverse_transform(threshold.reshape(-1, len(X_COLUMNS))).reshape(threshold.shape)\n",
    "#         quantile = X_scaler.inverse_transform(quantile.reshape(-1, len(X_COLUMNS))).reshape(quantile.shape)\n",
    "#     else:\n",
    "#         control = X_scaler.transform(control.reshape(-1, len(X_COLUMNS))).reshape(control.shape)\n",
    "#         x0 = X_scaler.transform(x0.reshape(-1, len(X_COLUMNS))).reshape(x0.shape)\n",
    "#         u_vars = U_scaler.transform(u_vars.reshape(-1, len(U_COLUMNS))).reshape(u_vars.shape)\n",
    "#         true = X_scaler.transform(true.reshape(-1, len(X_COLUMNS))).reshape(true.shape)\n",
    "        \n",
    "#     x_prefix = \"\" if rescale else \"Scaled \"\n",
    "#     u_prefix = \"\" if rescale else \"Scaled \"\n",
    "#     units = \"\" if rescale else \"Scaled \"\n",
    "    \n",
    "#     with plt.rc_context({\"xtick.labelsize\": 16, \"ytick.labelsize\": 16, \"axes.labelsize\": 16, \"axes.titlesize\": 16}):\n",
    "#         fig, axs = plt.subplots(12, 4, figsize=figsize)\n",
    "#         for i, ax in enumerate(axs.flatten()):\n",
    "#             x_coords = np.arange(true.shape[0])+1\n",
    "#             # X\n",
    "#             if i < 31:\n",
    "#                 # X\n",
    "#                 label = sorted(X_COLUMNS_NAMES)[i]\n",
    "#                 label_index = X_COLUMNS_NAMES.index(label)\n",
    "#                 ax.plot(x_coords, true[:, label_index], \"k-\")\n",
    "#                 ax.scatter(0, x0[label_index], c=\"k\", marker=\"o\")\n",
    "#                 ax.plot(x_coords, control[:, label_index])\n",
    "#                 ax.plot(x_coords, full[:, label_index])\n",
    "#                 ax.plot(x_coords, threshold[:, label_index])\n",
    "#                 ax.plot(x_coords, quantile[:, label_index])\n",
    "#                 ax.set_title(label)\n",
    "                \n",
    "#             elif (i > 31) and (i < 46):\n",
    "#                 # U\n",
    "#                 ii = i - 32\n",
    "#                 label = sorted(U_COLUMNS_NAMES)[ii]\n",
    "#                 label_index = U_COLUMNS_NAMES.index(label)\n",
    "#                 ax.plot(u_vars[:, label_index], \"k-\")\n",
    "#                 ax.set_title(label)\n",
    "            \n",
    "#             else:\n",
    "#                 ax.axis('off')\n",
    "#                 continue\n",
    "            \n",
    "#         fig.text(-0.00, 8/12, ha=\"center\", va=\"center\", rotation=90, s=f\"{x_prefix}Process Variables\", fontsize=16, fontweight=\"bold\")\n",
    "#         fig.text(-0.00, 2/12, ha=\"center\", va=\"center\", rotation=90, s=f\"{u_prefix}Exogenous Inputs\", fontsize=16, fontweight=\"bold\")\n",
    "#         fig.text(0.5, 0, ha=\"center\", va=\"center\", s=\"Prediction Horizon\", fontsize=16, fontweight=\"bold\")\n",
    "#         fig.text(0.5, 0.7/12, ha=\"left\", va=\"center\", s=f\"Prediction index: {index} of {logger.get_dataset(control_group+'Y').squeeze().shape[0]}\", fontsize=14)\n",
    "#         # fig.text(0.5, 0.5/12, ha=\"left\", va=\"center\", s=f\"Prediction {units}MAE: {nMAE:4g}\", fontsize=14)\n",
    "\n",
    "#         fig.tight_layout()\n",
    "#         fig.patches.extend([plt.Rectangle(xy=(-0.01,-0.2/12), width=1.01, height=4.2/12, fill=True, color=\"lightgrey\", zorder=-1, transform=fig.transFigure, figure=fig)])\n",
    "            \n",
    "#         # fig.subplots_adjust(hspace=0.9)\n",
    "#         fig.text(0.5, 1.00, f\"{key}: Prediction Comparison\", ha=\"center\", va=\"center\", fontsize=20, fontweight=\"bold\")\n",
    "#         fig.legend([\"True\", \"Initial Value\", \"No Updates\", \"Full Updates\", \"Threshold Updates\", \"Quantile Updates\"], bbox_to_anchor=(3.1/4,5/12), loc=\"upper left\", ncol=2)\n",
    "#         fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UpdatingNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
